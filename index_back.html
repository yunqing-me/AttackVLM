
<!DOCTYPE html>
<html>

<head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-XB3PR2Y1TQ"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-XB3PR2Y1TQ');
    </script>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, shrink-to-fit=no">
    <title>DreamFusion: Text-to-3D using 2D Diffusion</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.5.0/css/bootstrap.min.css">
    <link href='https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,500,600' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="/assets/css/Highlight-Clean.css">
    <link rel="stylesheet" href="/assets/css/styles.css">

    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">

    <meta property="og:site_name" content="DreamFusion: Text-to-3D using 2D Diffusion" />
    <meta property="og:type" content="video.other" />
    <meta property="og:title" content="DreamFusion: Text-to-3D using 2D Diffusion" />
    <meta property="og:description" content="DreamFusion: Text-to-3D using 2D Diffusion, 2022." />
    <meta property="og:url" content="https://dreamfusion3d.github.io/" />
    <meta property="og:image" content="https://dreamfusion3d.github.io/assets/images/dreamfusion_samples.png" />

    <meta property="article:publisher" content="https://dreamfusion3d.github.io/" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="DreamFusion: Text-to-3D using 2D Diffusion" />
    <meta name="twitter:description" content="We combine neural rendering with a multi-modal text-to-2D image diffusion generative model to synthesize diverse 3D objects from text." />
    <meta name="twitter:url" content="https://dreamfusion3d.github.io/" />
    <meta name="twitter:image" content="https://dreamfusion3d.github.io/assets/images/dreamfusion_samples.png" />
    <!-- <meta name="twitter:site" content="" /> -->

    <script src="assets/js/video_comparison.js"></script>
    <script type="module" src="https://unpkg.com/@google/model-viewer@2.0.1/dist/model-viewer.min.js"></script>
</head>

<body>
    <div class="banner">
      <!-- <video class="video lazy"
          poster="https://dreamfusion-cdn.ajayj.com/sept28/banner_1x6_customhue_A.jpg"
          autoplay loop playsinline muted>
        <source data-src="https://dreamfusion-cdn.ajayj.com/sept28/banner_1x6_customhue_A.mp4" type="video/mp4"></source>
      </video> -->
    </div>
    <div class="highlight-clean" style="padding-bottom: 10px;">
        <div class="container" style="max-width: 900px; margin-top: 50px;">
            <h1 class="text-center">On Evaluating Adversarial Robustness of <br> Large Vision-Language Models</h1>
        </div>
        <div class="container" style="max-width: 950px; margin-top: 35px; margin-bottom: 35px;">
            <div class="row authors">
                <div class="col-sm-3">
                    <h5 class="text-center"><a class="text-center" href="https://cs.stanford.edu/~poole/">Yunqing Zhao<sup>&#10038;</sup></a></h5>
                    <h6 class="text-center">SUTD, Singapore</h6>
                </div>
                <div class="col-sm-3">
                    <h5 class="text-center"><a href="https://ajayj.com">Tianyu Pang<sup>&#10038;&#9768;</sup></a></h5>
                    <h6 class="text-center">Sea AI Lab, Singapore</h6>
                </div>
                <div class="col-sm-3">
                    <h5 class="text-center"><a class="text-center" href="https://jonbarron.info/">Chao Du<sup>&#9768;</sup></a></h5>
                    <h6 class="text-center">Sea AI Lab, Singapore</h6>
                </div>
                <div class="col-sm-3">
                    <h5 class="text-center"><a class="text-center" href="https://bmild.github.io/">Xiao Yang</a></h5>
                    <h6 class="text-center">Tsinghua University</h6>
                </div> 
                <br>
                <div class="col-sm-3">
                    <h5 class="text-center"><a class="text-center" href="https://bmild.github.io/">Chongxuan Li</a></h5>
                    <h6 class="text-center">Renmin University of China</h6>
                </div> 
                <div class="col-sm-3">
                    <h5 class="text-center"><a class="text-center" href="https://bmild.github.io/">Ngai-Man Cheung<sup>&#9768;</sup></a></h5>
                    <h6 class="text-center">SUTD, Singapore</h6>
                </div> 
                <div class="col-sm-3">
                    <h5 class="text-center"><a class="text-center" href="https://bmild.github.io/">Min Lin</a></h5>
                    <h6 class="text-center">Sea AI Lab, Singapore</h6>
                </div> 
                <div class="col-sm-3">
                    <h6 class="text-center"> (&#10038;) Equal Contribution</h6>
                    <h6 class="text-center"> (&#9768;) Equal Advise</h6>
                </div> 
            </div>    
        </div>
        <div class="buttons" style="margin-top: 20px; margin-bottom: 10px;">
            <a class="btn btn-light" role="button" href="https://arxiv.org/abs/2209.14988">
                <svg style="width:24px;height:24px;margin-left:-12px;margin-right:12px" viewBox="0 0 24 24">
                    <path fill="currentColor" d="M16 0H8C6.9 0 6 .9 6 2V18C6 19.1 6.9 20 8 20H20C21.1 20 22 19.1 22 18V6L16 0M20 18H8V2H15V7H20V18M4 4V22H20V24H4C2.9 24 2 23.1 2 22V4H4M10 10V12H18V10H10M10 14V16H15V14H10Z"></path>
                </svg>Paper
            </a>
            <a class="btn btn-light disabled border border-dark" aria-disabled="true" role="button" href="#">
                <svg style="visibility:hidden;width:0px;height:24px;margin-left:-12px;margin-right:12px" width="0px" height="24px" viewBox="0 0 375 531">
                    <polygon stroke="#000000" points="0.5,0.866 459.5,265.87 0.5,530.874 "/>
                </svg>
                Project
            </a>
            <a class="btn btn-light" role="button" href="/gallery.html">
                <svg style="width:24px;height:24px;margin-left:-12px;margin-right:12px" width="24px" height="24px" viewBox="0 0 375 531">
                    <polygon stroke="#000000" points="0.5,0.866 459.5,265.87 0.5,530.874 "/>
                </svg>
                Gallery
            </a>
        </div>
    </div>
    <hr class="divider" />
    <div class="container" style="max-width: 768px;">
        <div class="row">
            <div class="col-md-12">
                <h2>Abstract</h2>
                <p>
                    <!-- <strong> -->
                        Large vision-language models (VLMs) such as GPT-4 have achieved unprecedented performance in response generation, especially with visual inputs, enabling more creative and adaptable interaction than large language models such as ChatGPT. Nonetheless, multimodal generation exacerbates safety concerns, since adversaries may successfully evade the entire system by subtly manipulating the most vulnerable modality (e.g., vision). To this end, we propose evaluating the robustness of open-source large VLMs in the most realistic and high-risk setting, where adversaries have only black-box system access and seek to deceive the model into returning the targeted} responses. In particular, we first craft targeted adversarial examples against pretrained models such as CLIP and BLIP, and then transfer these adversarial examples to other VLMs such as MiniGPT-4, LLaVA, UniDiffuser, BLIP-2, and Img2Prompt. In addition, we observe that black-box queries on these VLMs can further improve the effectiveness of targeted evasion, resulting in a surprisingly high success rate for generating targeted responses. Our findings provide a quantitative understanding regarding the adversarial vulnerability of large VLMs and call for a more thorough examination of their potential security flaws before deployment in practice.
                    <!-- </strong> -->
                </p>
            </div>
        </div>
    </div>
    <div class="container" style="max-width: 768px;">
        <div class="row captioned_videos">
            <div class="col-md-12">
                <!-- Large format devices -->
                <video class="video lazy d-none d-xs-none d-sm-block" autoplay loop playsinline muted poster="https://dreamfusion-cdn.ajayj.com/sept28/wipe_opposite_6x4_smoothstep.jpg">
                    <source data-src="https://dreamfusion-cdn.ajayj.com/sept28/wipe_opposite_6x4_smoothstep.mp4" type="video/mp4"></source>
                </video>
                <!-- Small format devices -->
                <video class="video lazy d-xs-block d-sm-none" autoplay loop playsinline muted poster="https://dreamfusion-cdn.ajayj.com/sept28/shaded_3x3_smoothstep.jpg">
                    <source data-src="https://dreamfusion-cdn.ajayj.com/sept28/shaded_3x3_smoothstep.mp4" type="video/mp4"></source>
                </video>
                <h6 class="caption">Given a caption, DreamFusion generates relightable 3D objects with high-fidelity appearance, depth, and normals. Objects are represented as a Neural Radiance Field and leverage a pretrained text-to-image diffusion prior such as Imagen.</h6>
            </div>
        </div>
    </div>
    <hr class="divider" />
    <div class="container" style="max-width: 768px;">
        <div class="row">
            <div class="col-md-12">
                <h2>Generate 3D from text yourself!</h2>
            </div>
        </div>
        <div class="row compositional captioned_videos">
            <div class="col-sm-8 text">
                <p class="selectable left" id="compositional_tags_depth_0"></p>
            </div>
            <div class="col-sm-4 my-auto">
                <div class="video-compare-container">
                    <video id="compositionalVideo" class="video" autoplay loop playsinline muted>
                        <div class="screen" id="compositionalScreen"></div>
                        <source id="compositionalVideoSrc" src="https://dreamfusion-cdn.ajayj.com/journey_sept28/cropped/full_continuous/a_DSLR_photo_of_a_squirrel___rgbdn_hq_15000.mp4" type="video/mp4">
                    </video>
                    <!-- <video onplay="resizeAndPlay(this)" style="height: 0px;" id="compositionalVideo" class="video lazy" autoplay loop playsinline muted>
                        <source id="compositionalVideoSrc" data-src="https://dreamfusion-cdn.ajayj.com/journey_sept28/full/a_DSLR_photo_of_a_squirrel___rgbdn_hq_15000.mp4" type="video/mp4">
                    </video> -->
                    <!-- <canvas height="752" class="videoMerge" id="compositionalVideoMerge" width="1002"></canvas> -->
                </div>
                <!-- <h6 class="caption" id="compositionalCaption">a DSLR photo of a squirrel</h6> -->
            </div>
        </div>
    </div>
    <hr class="divider" />
    <div class="container" style="max-width: 768px;">
        <div class="row">
            <div class="col-sm-8">
                <h2>Example generated objects</h2>
                <p>DreamFusion generates objects and scenes from diverse captions. <a href="/gallery.html">Search through hundreds of generated assets in our full gallery.</a></p>
            </div>
            <div class="col-sm-4 my-auto">
                <a href="/gallery.html" class="btn btn-primary btn-lg btn-search"><svg xmlns="http://www.w3.org/2000/svg" width="24px" height="24px" viewbox="0 0 600 550">
                    <path fill="none" stroke="#fff" stroke-width="36" stroke-linecap="round" d="m280,278a153,153 0 1,0-2,2l170,170m-91-117 110,110-26,26-110-110"/>
                    </svg>Search assets
                </a>
            </div>
        </div>
        <div class="row captioned_videos" id="randomVideos">
            <!-- <div class="col-4">
                <div class="video-compare-container" style="width: 100%">
                    <video class="video lazy" id="ex1" loop playsinline autoplay muted onplay="resizeAndPlay(this)" style="height: 0px;">
                        <source data-src="https://dreamfusion-cdn.ajayj.com/gallery/a_teddy_bear_pushing_a_shopping_cart_full_of_fruits_and_vegetables_rgbdn_hq_15000.mp4" type="video/mp4"></source>
                    </video>
                    <canvas height="752" class="videoMerge" id="ex1Merge" width="1002"></canvas>
                </div>
                <h6 class="caption">A teddy bear pushing a shopping cart full of fruits and vegetables.</h6>
            </div>
            <div class="col-4">
                <div class="video-compare-container" style="width: 100%">
                    <video class="video lazy" id="ex2" loop playsinline autoplay muted onplay="resizeAndPlay(this)" style="height: 0px;">
                        <source data-src="https://dreamfusion-cdn.ajayj.com/gallery/a_sliced_loaf_of_fresh_bread_rgbdn_hq_15000.mp4" type="video/mp4"></source>
                    </video>
                    <canvas height="752" class="videoMerge" id="ex2Merge" width="1002"></canvas>
                </div>
                <h6 class="caption">a sliced loaf of fresh bread.</h6>
            </div>
            <div class="col-4">
                <div class="video-compare-container" style="width: 100%">
                    <video class="video lazy" id="ex3" loop playsinline autoplay muted onplay="resizeAndPlay(this)" style="height: 0px;">
                        <source data-src="https://dreamfusion-cdn.ajayj.com/gallery/a_zoomed_out_DSLR_photo_of_Sydney_opera_house,_aerial_view_rgbdn_hq_15000.mp4" type="video/mp4"></source>
                    </video>
                    <canvas height="752" class="videoMerge" id="ex3Merge" width="1002"></canvas>
                </div>
                <h6 class="caption">a zoomed out DSLR photo of Sydney opera house, aerial view.</h6>
            </div> -->
        </div>
    </div>
    <hr class="divider" />
    <div class="container" style="max-width: 768px;">
        <div class="row">
            <div class="col-md-12">
                <h2>Composing objects into a scene</h2>
                <!-- <p>Our generated NeRF models can be exported to meshes using the marching cubes algorithm for easy integration into 3D renderers or modeling software.</p> -->
                <video class="video lazy" autoplay loop playsinline controls muted poster="https://dreamfusion-cdn.ajayj.com/carouselx24_128tall.jpg">
                    <source src="https://dreamfusion-cdn.ajayj.com/carouselx24_128tall.mp4" type="video/mp4"></source>
                </video>
            </div>
        </div>
    </div>
    <hr class="divider" />
    <div class="container meshes" id="meshContainer" style="max-width: 768px;">
        <div class="row">
            <div class="col-md-12">
                <h2>Mesh exports</h2>
                <p>Our generated NeRF models can be exported to meshes using the marching cubes algorithm for easy integration into 3D renderers or modeling software.</p>
            </div>
        </div>
    </div>
    <hr class="divider" />
    <div class="container" style="max-width: 768px;">
        <div class="row">
            <div class="col-md-12">
                <h2>How does DreamFusion work?</h2>
                <p>Given a caption, DreamFusion uses a text-to-image generative model called Imagen to optimize a 3D scene. We propose <strong>Score Distillation Sampling (SDS)</strong>, a way to generate samples from a diffusion model by optimizing a loss function. SDS allows us to optimize samples in an arbitrary parameter space, such as a 3D space, as long as we can map back to images differentiably. We use a 3D scene parameterization similar to Neural Radiance Fields, or NeRFs, to define this differentiable mapping. SDS alone produces reasonable scene appearance, but DreamFusion adds additional regularizers and optimization strategies to improve geometry. The resulting trained NeRFs are coherent, with high-quality normals, surface geometry and depth, and are relightable with a Lambertian shading model.</p>
            </div>
        </div>
        <div class="row">
            <div class="col-md-12">
                <video class="video lazy" controls muted poster="https://dreamfusion-cdn.ajayj.com/dreamfusion_overview.jpg">
                    <source data-src="https://dreamfusion-cdn.ajayj.com/dreamfusion_overview.mp4" type="video/mp4"></source>
                </video>
            </div>
        </div>
    </div>
    <hr class="divider" />
    <div class="container" style="max-width: 768px;">
        <div class="row">
            <div class="col-md-12">
                <h2>Citation</h2>
                <code>
                    @article{poole2022dreamfusion,<br>
                    &nbsp; author = {Poole, Ben and Jain, Ajay and Barron, Jonathan T. and Mildenhall, Ben},<br>
                    &nbsp; title  = {DreamFusion: Text-to-3D using 2D Diffusion},<br>
                    &nbsp; journal = {arXiv},<br>
                    &nbsp; year   = {2022},<br>
                }</code>
            </div>
        </div>
    </div>
    <script src="https://polyfill.io/v3/polyfill.js?features=IntersectionObserver"></script>
    <script src="/assets/js/yall.js"></script>
    <script>
        yall(
            {
                observeChanges: true
            }
        );
    </script>
    <script src="/assets/js/scripts.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.5.0/js/bootstrap.bundle.min.js"></script>
    <script src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/js/webflow.fd002feec.js"></script>
    <!-- Import the component -->
</body>

</html>
